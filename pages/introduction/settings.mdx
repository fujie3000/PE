# LLM 设置

当使用提示时，您将通过 API 或直接与 LLM 进行交互。您可以配置一些参数以获取不同的提示结果。

**温度** - 简而言之，温度越低，结果越确定，因为总是选择最可能的下一个标记。增加温度可能会带来更多的随机性，从而鼓励更多不同或创造性的输出。我们实质上是增加了其他可能标记的权重。在应用方面，我们可能希望对例如基于事实的问答这样的任务使用较低的温度值，以鼓励更加准确和简洁的回答。对于生成诗歌或其他创造性任务，增加温度值可能是有益的。

**Top_p** - 同样地，使用 `top_p`，一种具有温度的采样技术叫做 nucleus sampling，您可以控制模型在生成响应时的确定性。如果您正在寻找确切和事实答案，请将其保持较低。如果您正在寻找更多样化的答案，将其增加到较高值。

一般推荐只更改其中一项。

在开始一些基本示例之前，请记住，您的结果可能会因使用的 LLM 版本而异。