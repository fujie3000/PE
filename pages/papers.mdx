# 论文

以下是有关提示工程的最新论文（按发布日期排序）。我们每天更新，新的论文不断涌现。每周我们会将这些论文的摘要纳入指南中。

## 概述

- [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)（2023年2月）
- [A Survey for In-context Learning](https://arxiv.org/abs/2301.00234)（2022年12月）
- [Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/abs/2212.10403)（2022年12月）
- [Reasoning with Language Model Prompting: A Survey](https://arxiv.org/abs/2212.09597)（2022年12月）
- [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)（2022年6月）
- [A Taxonomy of Prompt Modifiers for Text-To-Image Generation](https://arxiv.org/abs/2204.13988)（2022年4月）
- [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/abs/2107.13586)（2021年7月）

## 方法

- [Visual-Language Prompt Tuning with Knowledge-guided Context Optimization](https://arxiv.org/abs/2303.13283)（2023年3月）
- [Fairness-guided Few-shot Prompting for Large Language Models](https://arxiv.org/abs/2303.13217)（2023年3月）
- [Context-faithful Prompting for Large Language Models](https://arxiv.org/abs/2303.11315)（2023年3月）
- [Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning](https://arxiv.org/abs/2303.10475)（2023年3月）
- [UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation](https://arxiv.org/abs/2303.08518)（2023年3月）
- [Model-tuning Via Prompts Makes NLP Models Adversarially Robust](https://arxiv.org/abs/2303.07320)（2023年3月）
- [Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer](https://arxiv.org/abs/2303.03922)（2023年3月）
- [CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification](https://arxiv.org/abs/2303.03628)（2023年3月）- [更大的语言模型以不同的方式进行上下文学习](https://arxiv.org/abs/2303.03846) (2023 年 3 月)
  - [OpenICL: 一种开源的上下文学习框架](https://arxiv.org/abs/2303.02913) (2023 年 3 月)
  - [动态提示：统一的提示调整框架](https://arxiv.org/abs/2303.02909) (2023 年 3 月)
  - [多任务 prompt 调整实现参数高效的迁移学习](https://arxiv.org/abs/2303.02861) (2023 年 3 月)
  - [数据增强技术在有限数据下的前缀调整效果研究](https://arxiv.org/abs/2303.02577) (2023 年 3 月)
  - [软提示混合用于可控数据生成](https://arxiv.org/abs/2303.01580) (2023 年 3 月)
  - [Prompt, Generate, then Cache: 基础模型的级联使得强大的 few-shot 学习者](https://arxiv.org/abs/2303.02151) (2023 年 3 月)
  - [GPT-3.5 对前身的鲁棒性如何？语言理解任务综合研究](https://arxiv.org/abs/2303.00293) (2023 年 3 月)
  - [ChatGPT 能够理解吗？ChatGPT 和微调的 BERT 的比较研究](https://arxiv.org/pdf/2302.10198.pdf) (2023 年 2 月)
  - [EvoPrompting：面向代码级神经架构搜索的语言模型](https://arxiv.org/abs/2302.14838) (2023 年 2 月)
  - [上下文指令学习](https://arxiv.org/abs/2302.14691) (2023 年 2 月)
  - [Chain of Hindsight 将语言模型与反馈对齐](https://arxiv.org/abs/2302.02676) (2023 年 2 月)
  - [语言并不代表一切：将知觉与语言模型对齐](https://arxiv.org/abs/2302.14045) (2023 年 2 月)
  - [从标记数据中通过连续思考进行自动提示增强和选择](https://arxiv.org/abs/2302.12822) (2023 年 2 月)
  - [大型语言模型的连续思考主动提示](https://arxiv.org/abs/2302.12246) (2023 年 2 月)
  - [不仅仅是你要求的：大型应用集成的语言模型新型提示注入攻击的综合分析](https://arxiv.org/abs/2302.12173) (2023 年 2 月)- [使用Prompt模式目录增强基于ChatGPT的提示工程](https://arxiv.org/abs/2302.11382)（2023年2月）
  - [通过方向刺激提示引导大型语言模型]( https://arxiv.org/abs/2302.11520)（2023年2月）
  - [上下文学习如何帮助提示调优？]( https://arxiv.org/abs/2302.11521)（2023年2月）
  - [可伸缩的Prompt生成，用于半监督学习与语言模型](https://arxiv.org/abs/2302.09236)（2023年2月）
  - [通过Prompt约束限制大型语言模型的开放文本生成能力]( https://arxiv.org/abs/2302.09185)（2023年2月）
  - [定制Prompt调优（APT）：通过组合提示结合不同的数据]( https://arxiv.org/abs/2302.07994)（2023年2月）
  - [GraphPrompt：将预训练和下游任务统一为图神经网络]( https://arxiv.org/abs/2302.08043)（2023年2月）
  - [大型语言模型的道德自我修正能力]( https://arxiv.org/abs/2302.07459)（2023年2月）
  - [SwitchPrompt：学习特定于领域的门控软Prompt用于低资源领域的分类]( https://arxiv.org/abs/2302.06868)（2023年2月）
  - [评估离散提示的鲁棒性]( https://arxiv.org/abs/2302.05619)（2023年2月）
  - [上下文学习的组成范例]( https://arxiv.org/abs/2302.05698)（2023年2月）
  - [Hard Prompt变简单：面向Prompt调优和发现的基于梯度的离散优化]( https://arxiv.org/abs/2302.03668)（2023年2月）
  - [大型语言模型能够很容易地被无关上下文所分散]( https://arxiv.org/abs/2302.00093)（2023年2月）
  - [合成提示：为大型语言模型生成链式思考演示]( https://arxiv.org/abs/2302.00618)（2023年2月）
  - [渐进提示：语言模型的持续学习]( https://arxiv.org/abs/2301.12314)（2023年1月）- [批量提示：利用LLM API实现高效推理](https://arxiv.org/abs/2301.08721) (2023年1月)
  - [展示-搜索-预测：组合检索和语言模型进行知识密集型NLP](https://arxiv.org/abs/2212.14024) (2022年12月)
  - [再三思考，我们不必逐步思考！零点推理中的偏见和毒性](https://arxiv.org/abs/2212.08061) (2022年12月)
  - [宪法AI：AI反馈的无害性](https://arxiv.org/abs/2212.08073) (2022年12月)
  - [连续提示分解复杂问题](https://arxiv.org/abs/2212.04092) (2022年12月)
  - [大型语言模型是具有自我验证能力的推理机](https://arxiv.org/abs/2212.09561v1) (2022年12月)
  - [通过模型编写的评估发现语言模型行为](https://arxiv.org/abs/2212.09251) (2022年12月)
  - [结构化提示：将上下文学习扩展到1,000个示例](https://arxiv.org/abs/2212.06713) (2022年12月)
  - [PAL：程序辅助的语言模型](https://arxiv.org/abs/2211.10435) (2022年11月)
  - [大型语言模型是人类级别的提示工程师](https://arxiv.org/abs/2211.01910) (2022年11月)
  - [忽略先前提示：面向语言模型的攻击技术](https://arxiv.org/abs/2211.09527) (2022年11月)
  - [机器生成的文本：威胁模型和检测方法的全面调查](https://arxiv.org/abs/2210.07321) (2022年11月)
  - [通过上下文学习教授算法推理](https://arxiv.org/abs/2211.09066) (2022年11月)
  - [通过自然语言推理增强预先训练的语言模型的自我一致性和性能](https://arxiv.org/abs/2211.11875) (2022年11月)
  - [问我任何事：提示语言模型的简单策略](https://paperswithcode.com/paper/ask-me-anything-a-simple-strategy-for) (2022年10月)
  - [复述增强的语言模型](https://arxiv.org/abs/2210.01296) (2022年10月)
  - [ReAct：在语言模型中协同推理和行动](https://arxiv.org/abs/2210.03629) (2022年10月)- [Prompting GPT-3 To Be Reliable](https://arxiv.org/abs/2210.09150)（2022年10月）
  - [Decomposed Prompting: A Modular Approach for Solving Complex Tasks](https://arxiv.org/abs/2210.02406)（2022年10月）
  - [Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought](https://arxiv.org/abs/2210.01240v3)（2022年10月）
  - [Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples](https://arxiv.org/abs/2209.02128)（2022年9月）
  - [Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning](https://arxiv.org/abs/2209.14610)（2022年9月）
  - [Promptagator: Few-shot Dense Retrieval From 8 Examples](https://arxiv.org/abs/2209.11755)（2022年9月）
  - [Atlas: Few-shot Learning with Retrieval Augmented Language Models](https://arxiv.org/abs/2208.03299)（2022年11月）
  - [DocPrompting: Generating Code by Retrieving the Docs](https://arxiv.org/abs/2207.05987)（2022年7月）
  - [On the Advance of Making Language Models Better Reasoners](https://arxiv.org/abs/2206.02336)（2022年6月）
  - [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)（2022年5月）
  - [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://arxiv.org/abs/2205.11822)（2022年5月）
  - [MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/abs/2205.00445)（2022年5月）
  - [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://aclanthology.org/2022.acl-long.576/)（2022年5月）
  - [Toxicity Detection with Generative Prompt-based Inference](https://arxiv.org/abs/2205.12390)（2022年5月）
  - [Learning to Transfer Prompts for Text Generation](https://arxiv.org/abs/2205.01543)（2022年5月）
  - [The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning](https://arxiv.org/abs/2205.03401)（2022年5月）
  - [A Taxonomy of Prompt Modifiers for Text-To-Image Generation](https://arxiv.org/abs/2204.13988)（2022年4月）- [PromptChainer: 通过可视编程链式连接大型语言模型提示](https://arxiv.org/abs/2203.06566) （2022年3月）
  - [自洽性提高了大型语言模型的思维链推理能力](https://arxiv.org/abs/2203.11171) （2022年3月）
  - [通过人类反馈训练语言模型遵循指示](https://arxiv.org/abs/2203.02155)
  - [重新思考演示的角色：什么使得上下文学习起作用？](https://arxiv.org/abs/2202.12837) （2022年2月）
  - [思维链提示引发大型语言模型的推理](https://arxiv.org/abs/2201.11903) （2022年1月）
  - [展示你的工作：用于与语言模型进行中间计算的草稿板](https://arxiv.org/abs/2112.00114) （2021年11月）
  - [AI Chains：通过链接大型语言模型提示的透明且可控人机交互](https://arxiv.org/abs/2110.01691) （2021年10月）
  - [生成的知识提示用于常识推理](https://arxiv.org/abs/2110.08387) （2021年10月）
  - [多任务提示训练使零样本任务泛化成为可能](https://arxiv.org/abs/2110.08207) （2021年10月）
  - [重构GPTk的语言以进行指示提示](https://arxiv.org/abs/2109.07830) （2021年9月）
  - [设计提示工程文本到图像生成模型的指南](https://arxiv.org/abs/2109.06977) （2021年9月）
  - [让预训练语言模型成为更好的少样本学习器](https://aclanthology.org/2021.acl-long.295) （2021年8月）
  - [奇妙有序的提示及其寻找之处：克服少样本提示顺序敏感性](https://arxiv.org/abs/2104.08786) （2021年4月）
  - [BERTese：学会与BERT交流](https://aclanthology.org/2021.eacl-main.316) （2021年4月）
  - [规模在参数高效提示调优方面的作用](https://arxiv.org/abs/2104.08691) （2021年4月）
  - [大型语言模型的提示编程：超越少样本模式](https://arxiv.org/abs/2102.07350) （2021年2月）- [在使用前进行校准：改善语言模型的少样本性能](https://arxiv.org/abs/2102.09690) (2021年2月)
  - [前缀调整：优化生成的连续提示](https://arxiv.org/abs/2101.00190) (2021年1月)
  - [从任务描述中学习生成任务特定适配器](https://arxiv.org/abs/2101.00420) (2021年1月)
  - [使预训练语言模型成为更好的少样本学习者](https://arxiv.org/abs/2012.15723) (2020年12月)
  - [从任务描述中学习](https://aclanthology.org/2020.emnlp-main.105/) (2020年11月)
  - [AutoPrompt：使用自动生成的提示从语言模型中引出知识](https://arxiv.org/abs/2010.15980) (2020年10月)
  - [语言模型是少样本学习者](https://arxiv.org/abs/2005.14165) (2020年5月)
  - [我们如何知道语言模型知道什么？](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00324/96460/How-Can-We-Know-What-Language-Models-Know) (2020年7月)

## 应用

  - [SPeC：一种基于软提示的校准方法，用于减轻临床记录摘要的性能差异](https://arxiv.org/abs/2303.13035) (2023年3月)
  - [大型语言模型和简单的愚蠢错误](https://arxiv.org/abs/2303.11455) (2023年3月)
  - [生成式预训练转换器（GPT）能否通过高等教育编程课程的评估？](https://arxiv.org/abs/2303.09325) (2023年3月)
  - [SelfCheckGPT：零资源黑匣子幻觉检测生成式大型语言模型](https://arxiv.org/abs/2303.08896) (2023年3月)
  - [ICL-D3IE：文档信息抽取的上下文学习和多样示范更新](https://arxiv.org/abs/2303.05063) (2023年3月)
  - [MathPrompter：使用大型语言模型进行数学推理](https://arxiv.org/abs/2303.05398) (2023年3月)
  - [基于提示的学习，用于预测网络安全论坛中的线程结构](https://arxiv.org/abs/2303.05400) (2023年3月)- [Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting](https://arxiv.org/abs/2303.03199) (2023年3月)
  - [使用基于知识的视觉问答候选者启发方式促进大型语言模型回答问题](https://arxiv.org/abs/2303.01903) (2023年3月)
  - [软启发引导的跨领域情感分析联合学习](https://arxiv.org/abs/2303.00815) (2023年3月)
  - [SpeechPrompt v2：针对语音分类任务的提示调整](https://arxiv.org/abs/2303.00733) (2023年3月)
  - [通过语言描述实现基于目标的分布差异发现](https://arxiv.org/abs/2302.14233) (2023年2月)
  - [在语言模型中导航灰色区域：过度自信和不确定性的表达](https://arxiv.org/abs/2302.13439) (2023年2月)
  - [TabGenie：表格到文本生成的工具包](https://arxiv.org/abs/2302.14169) (2023年2月)
  - [SGL-PT：具有图形提示调整的强大图形学习器](https://arxiv.org/abs/2302.12449) (2023年2月)
  - [基于提示的适配器的少量样本表格到文本生成](https://arxiv.org/abs/2302.12468) (2023年2月)
  - [语言模型是预后预测的少量样本学习器](https://arxiv.org/abs/2302.12692) (2023年2月)
  - [STA：自控文本增强以改善文本分类](https://arxiv.org/abs/2302.12784) (2023年2月)
  - [检查你的事实并重试：使用外部知识和自动反馈改进大型语言模型](https://arxiv.org/abs/2302.12813) (2023年2月)
  - [生成式AI模型（如ChatGPT）如何被（误）用于SPC实践、教育和研究？一项探索性研究](https://arxiv.org/abs/2302.10916) (2023年2月) 
  - [Grimm in Wonderland：通过中途提示工程来说明童话故事](https://arxiv.org/abs/2302.08961) (2023年2月)
  - [LabelPrompt：用于关系分类的有效基于提示的学习](https://arxiv.org/abs/2302.08068) (2023年2月)- [语言模型交叉：通过少量提示进行变体](https://arxiv.org/abs/2302.09236)（2023年2月）
  - [深度神经网络的提示微调用于说话人自适应视觉语音识别](https://arxiv.org/abs/2302.08102)（2023年2月）
  - [大型语言模型的道德自我修正能力](https://arxiv.org/abs/2302.07459)（2023年2月）
  - [多模恶意模因分类的提示](https://arxiv.org/abs/2302.04156)（2023年2月）
  - [用于社交会话合成的提示语言模型（PLACES）](https://arxiv.org/abs/2302.03269)（2023年2月）
  - [常识感知提示对于可控移情对话生成的作用](https://arxiv.org/abs/2302.01441)（2023年2月）
  - [抓取语言模型的内部知识库](https://arxiv.org/abs/2301.12810)（2023年1月）
  - [面向多语种法律判断预测的法律提示工程](https://arxiv.org/abs/2212.02199)（2022年12月）
  - [扩散模型中的提示工程研究](https://arxiv.org/abs/2211.15462)（2022年11月）
  - [通过思路链学习解释：用于科学问答的多模态推理](https://arxiv.org/abs/2209.09513v2)（2022年9月）
  - [与Copilot对话：探索自然语言解决计算机科学问题的提示工程](https://arxiv.org/abs/2210.15157)（2022年10月）
  - [Copilot和Codex的试飞：高温度，低提示，还是魔法？](https://arxiv.org/abs/2210.14699)（2022年10月）
  - [从头开始为预训练语言模型编写情节](https://aclanthology.org/2022.inlg-main.5)（2022年7月）

## 收藏

  - [Chain-of-Thought Papers（思路链论文）](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers)
  - [Papers with Code（带有代码的论文）](https://paperswithcode.com/task/prompt-engineering)
  - [Prompt Papers（提示论文）](https://github.com/thunlp/PromptPapers#papers)